{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54fba428-1cb6-4d95-920d-c6e27d590d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenient for importing modules from the parent directory\n",
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ddd7462-37e4-493a-9d36-da78af660d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/aum/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import src.models as models\n",
    "from src import dataloader\n",
    "from src.utilities.stats import calculate_stats\n",
    "from IPython.display import Audio, display\n",
    "import csv\n",
    "import warnings\n",
    "\n",
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deb60b0c-4536-4c2f-acab-9a728fae4e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments about the data\n",
    "data_args = Namespace(\n",
    "    num_mel_bins = 128,\n",
    "    target_length = 1024,\n",
    "    mean = -5.0767093,\n",
    "    std = 4.4533687,\n",
    ")\n",
    "\n",
    "# Arguments about the model\n",
    "model_args = Namespace(\n",
    "    model_type = 'base',\n",
    "    n_classes = 309,\n",
    "    imagenet_pretrain = False,\n",
    "    imagenet_pretrain_path = None,\n",
    "    aum_pretrain = True,\n",
    "    aum_pretrain_path = 'models/aum-base_audioset-vggsound.pth',\n",
    "    aum_variant = 'Fo-Bi',\n",
    "    device = 'cuda',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32c443ff-f91d-4e15-8e43-00e16a1544cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n",
      "Resize function is resample_patch_embed\n",
      "Initializing FlexiPatchEmbed with the following parameters:\n",
      "patch_size=(16, 16), in_chans=1, embed_dim=768, bias=True, norm_layer=None, flatten=True, proj_load=yes, resize_func=resample_patch_embed\n",
      "The resize function is resample_patch_embed\n",
      "Loading projection weights!\n",
      "The shapes of the current projection: bias=torch.Size([768]), weight=torch.Size([768, 1, 16, 16])\n",
      "The shapes of the loaded projection: bias=torch.Size([768]), weight=torch.Size([768, 1, 16, 16])\n",
      "Initializing FlexiPosEmbed with the following parameters:\n",
      "input_size=(128, 1024), pos_grid_size=(8, 64), embed_dim=768, pos_embed_load=torch.Size([1, 513, 768]), pos_grid_size_load=(8, 64), n_prefix_tokens=1, pos_embed_prefix=True\n",
      "Loading position embedding!\n",
      "The shape of the current grid size: (8, 64)\n",
      "The shape of the loaded grid size: (8, 64)\n"
     ]
    }
   ],
   "source": [
    "# Initilize the model\n",
    "\n",
    "# Embedding dimension\n",
    "if 'base' in model_args.model_type:\n",
    "    embed_dim = 768\n",
    "elif 'small' in model_args.model_type:\n",
    "    embed_dim = 384\n",
    "elif 'tiny' in model_args.model_type:\n",
    "    embed_dim = 192\n",
    "\n",
    "# AuM block type\n",
    "bimamba_type = {\n",
    "    'Fo-Fo': 'none', \n",
    "    'Fo-Bi': 'v1', \n",
    "    'Bi-Bi': 'v2'\n",
    "}.get(\n",
    "    model_args.aum_variant, \n",
    "    None\n",
    ")\n",
    "\n",
    "AuM = models.AudioMamba(\n",
    "    spectrogram_size=(data_args.num_mel_bins, data_args.target_length),\n",
    "    patch_size=(16, 16),\n",
    "    strides=(16, 16),\n",
    "    embed_dim=embed_dim,\n",
    "    num_classes=model_args.n_classes,\n",
    "    imagenet_pretrain=model_args.imagenet_pretrain,\n",
    "    imagenet_pretrain_path=model_args.imagenet_pretrain_path,\n",
    "    aum_pretrain=model_args.aum_pretrain,\n",
    "    aum_pretrain_path=model_args.aum_pretrain_path,\n",
    "    bimamba_type=bimamba_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c55b0294-aeb9-48bd-8f37-de6702396d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n",
      "Resize function is resample_patch_embed\n",
      "Initializing FlexiPatchEmbed with the following parameters:\n",
      "patch_size=[16, 16], in_chans=1, embed_dim=768, bias=True, norm_layer=None, flatten=True, proj_load=yes, resize_func=resample_patch_embed\n",
      "The resize function is resample_patch_embed\n",
      "Loading projection weights!\n",
      "The shapes of the current projection: bias=torch.Size([768]), weight=torch.Size([768, 1, 16, 16])\n",
      "The shapes of the loaded projection: bias=torch.Size([768]), weight=torch.Size([768, 1, 16, 16])\n",
      "Initializing FlexiPosEmbed with the following parameters:\n",
      "input_size=[128, 1024], pos_grid_size=(8, 64), embed_dim=768, pos_embed_load=torch.Size([1, 513, 768]), pos_grid_size_load=(8, 64), n_prefix_tokens=1, pos_embed_prefix=True\n",
      "Loading position embedding!\n",
      "The shape of the current grid size: (8, 64)\n",
      "The shape of the loaded grid size: (8, 64)\n"
     ]
    }
   ],
   "source": [
    "model = AuM.from_pretrained(\"Robzy/audiomamba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5c34c00-af9f-402f-a72c-0a6fd23c05ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class AuMConfig(PretrainedConfig):\n",
    "    model_type = \"mamba\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_mel_bins: int = 128,\n",
    "        target_length: int = 1024,\n",
    "        mean: float = -5.0767093,\n",
    "        std: float = 4.4533687,\n",
    "        model_type: str = 'base',\n",
    "        n_classes: int = 309,\n",
    "        imagenet_pretrain: bool = False,\n",
    "        imagenet_pretrain_path: bool = None,\n",
    "        aum_pretrain: bool = True,\n",
    "        aum_pretrain_path: str = 'models/aum-base_audioset-vggsound.pth',\n",
    "        aum_variant: str = 'Fo-Bi',\n",
    "        device: str = 'cuda',\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        # Embedding dimension\n",
    "        if 'base' in model_type:\n",
    "            embed_dim = 768\n",
    "        elif 'small' in model_type:\n",
    "            embed_dim = 384\n",
    "        elif 'tiny' in model_type:\n",
    "            embed_dim = 192\n",
    "        \n",
    "        # AuM block type\n",
    "        bimamba_type = {\n",
    "            'Fo-Fo': 'none', \n",
    "            'Fo-Bi': 'v1', \n",
    "            'Bi-Bi': 'v2'\n",
    "        }.get(\n",
    "            aum_variant, \n",
    "            None\n",
    "        )\n",
    "            \n",
    "        self.spectrogram_size=(num_mel_bins, target_length)\n",
    "        self.patch_size=(16, 16)\n",
    "        self.strides=(16, 16)\n",
    "        self.embed_dim=embed_dim\n",
    "        self.num_classes=n_classes\n",
    "        self.imagenet_pretrain=imagenet_pretrain\n",
    "        self.imagenet_pretrain_path=imagenet_pretrain_path\n",
    "        self.aum_pretrain=aum_pretrain\n",
    "        self.aum_pretrain_path=aum_pretrain_path\n",
    "        self.bimamba_type=bimamba_type\n",
    "\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ff90177-9ca3-4b32-807a-8cba51d1d881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Robzy/audiomamba/commit/313271b51437dececf87e776dcabfbab29cb68ad', commit_message='Upload config', commit_description='', oid='313271b51437dececf87e776dcabfbab29cb68ad', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Robzy/audiomamba', endpoint='https://huggingface.co', repo_type='model', repo_id='Robzy/audiomamba'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AuMConfig()\n",
    "config.push_to_hub(repo_id=\"Robzy/audiomamba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0eb9479f-9951-4c65-ab7d-d09589e7f1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel\n",
    "from timm.models.resnet import BasicBlock, Bottleneck, ResNet\n",
    "\n",
    "class AudioMambaModel(PreTrainedModel):\n",
    "    config_class = AuMConfig()\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = models.AudioMamba(\n",
    "            spectrogram_size = config.spectrogram_size,\n",
    "            patch_size = config.patch_size,\n",
    "            strides = config.strides,\n",
    "            embed_dim = config.embed_dim,\n",
    "            num_classes = config.num_classes,\n",
    "            imagenet_pretrain = config.imagenet_pretrain,\n",
    "            imagenet_pretrain_path = config.imagenet_pretrain_path,\n",
    "            aum_pretrain = config.aum_pretrain,\n",
    "            aum_pretrain_path = config.aum_pretrain_path,\n",
    "            bimamba_type = config.bimamba_type,\n",
    "        )\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        return self.model(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa81fa00-8ad4-433f-a44e-fd11e3376b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n",
      "Resize function is resample_patch_embed\n",
      "Initializing FlexiPatchEmbed with the following parameters:\n",
      "patch_size=(16, 16), in_chans=1, embed_dim=768, bias=True, norm_layer=None, flatten=True, proj_load=yes, resize_func=resample_patch_embed\n",
      "The resize function is resample_patch_embed\n",
      "Loading projection weights!\n",
      "The shapes of the current projection: bias=torch.Size([768]), weight=torch.Size([768, 1, 16, 16])\n",
      "The shapes of the loaded projection: bias=torch.Size([768]), weight=torch.Size([768, 1, 16, 16])\n",
      "Initializing FlexiPosEmbed with the following parameters:\n",
      "input_size=(128, 1024), pos_grid_size=(8, 64), embed_dim=768, pos_embed_load=torch.Size([1, 513, 768]), pos_grid_size_load=(8, 64), n_prefix_tokens=1, pos_embed_prefix=True\n",
      "Loading position embedding!\n",
      "The shape of the current grid size: (8, 64)\n",
      "The shape of the loaded grid size: (8, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 368M/368M [00:22<00:00, 16.3MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Robzy/audiomamba/commit/c2585a1f1da62f8ea3fba91388e490f645fd8394', commit_message='Upload model', commit_description='', oid='c2585a1f1da62f8ea3fba91388e490f645fd8394', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Robzy/audiomamba', endpoint='https://huggingface.co', repo_type='model', repo_id='Robzy/audiomamba'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AudioMambaModel(config)\n",
    "model.push_to_hub(repo_id=\"Robzy/audiomamba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9d1d49d-8092-4e46-86a4-7a45dc217750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel, AutoModelForAudioClassification\n",
    "\n",
    "AutoConfig.register(\"audiomamba\", AutoConfig)\n",
    "AutoModel.register(AuMConfig, AutoConfig)\n",
    "AutoModelForAudioClassification.register(AuMConfig, AutoModelForAudioClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26174bdb-3f8a-4264-b935-90db03b109fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel, AutoModelForImageClassification\n",
    "\n",
    "AutoConfig.register(\"resnet\", ResnetConfig)\n",
    "AutoModel.register(ResnetConfig, ResnetModel)\n",
    "AutoModelForImageClassification.register(ResnetConfig, ResnetModelForImageClassification)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aum",
   "language": "python",
   "name": "aum"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
